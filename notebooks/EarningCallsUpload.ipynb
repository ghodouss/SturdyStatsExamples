{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ac1afc3b-a503-442b-8b75-49846e9618b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sturdy-stats-sdk pandas numpy plotly\n",
    "\n",
    "from sturdystats import Index, Job\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "API_KEY = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca0788-eb83-4e42-ae12-102825817be3",
   "metadata": {},
   "source": [
    "### Load Earnings Call Dataset\n",
    "\n",
    "For those who do not follow finance, an earnings call is  \"a conference call between the management of a public company, analysts, investors, and the media to discuss the companyâ€™s financial results during a given reporting period, such as a quarter or a fiscal year. An earnings call is usually preceded by an earnings report, which contains summary information on financial performance for the period.\" [1] \n",
    "\n",
    "This dataset is on earnings calls for the {Google, Apple, Meta, Microsoft, and Nvidia} for the years {2023, 2024}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f65aa643-72d7-4531-8b22-1355bc03fd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "      <th>doc</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>priceDelta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2024Q1</td>\n",
       "      <td>2023</td>\n",
       "      <td>Operator: Welcome, everyone. Thank you for sta...</td>\n",
       "      <td>2024-01-30</td>\n",
       "      <td>GOOG 2024Q1</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>-0.091346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2023Q4</td>\n",
       "      <td>2023</td>\n",
       "      <td>Operator: Welcome, everyone. Thank you for sta...</td>\n",
       "      <td>2023-10-24</td>\n",
       "      <td>GOOG 2023Q4</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>-0.083095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2023Q3</td>\n",
       "      <td>2023</td>\n",
       "      <td>Operator: Welcome, everyone. Thank you for sta...</td>\n",
       "      <td>2023-07-25</td>\n",
       "      <td>GOOG 2023Q3</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>0.061722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2023Q2</td>\n",
       "      <td>2023</td>\n",
       "      <td>Operator: Welcome, everyone. Thank you for sta...</td>\n",
       "      <td>2023-04-25</td>\n",
       "      <td>GOOG 2023Q2</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>-0.021557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2024Q4</td>\n",
       "      <td>2024</td>\n",
       "      <td>Operator: Welcome, everyone. Thank you for sta...</td>\n",
       "      <td>2024-10-29</td>\n",
       "      <td>GOOG 2024Q4</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>0.045372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker quarter  year                                                doc  \\\n",
       "0   GOOG  2024Q1  2023  Operator: Welcome, everyone. Thank you for sta...   \n",
       "1   GOOG  2023Q4  2023  Operator: Welcome, everyone. Thank you for sta...   \n",
       "2   GOOG  2023Q3  2023  Operator: Welcome, everyone. Thank you for sta...   \n",
       "3   GOOG  2023Q2  2023  Operator: Welcome, everyone. Thank you for sta...   \n",
       "4   GOOG  2024Q4  2024  Operator: Welcome, everyone. Thank you for sta...   \n",
       "\n",
       "    published        title author  priceDelta  \n",
       "0  2024-01-30  GOOG 2024Q1   GOOG   -0.091346  \n",
       "1  2023-10-24  GOOG 2023Q4   GOOG   -0.083095  \n",
       "2  2023-07-25  GOOG 2023Q3   GOOG    0.061722  \n",
       "3  2023-04-25  GOOG 2023Q2   GOOG   -0.021557  \n",
       "4  2024-10-29  GOOG 2024Q4   GOOG    0.045372  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"data/tech_earnings_calls_oct_2024.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02273150-288c-458a-ac6b-c5e041d3fd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['GOOG', 'AAPL', 'META', 'MSFT', 'NVDA'], dtype=object),\n",
       " array(['2022Q2', '2022Q3', '2022Q4', '2023Q1', '2023Q2', '2023Q3',\n",
       "        '2023Q4', '2024Q1', '2024Q2', '2024Q3', '2024Q4'], dtype='<U6'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ticker.unique(), np.array(sorted(df.quarter.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e70d4-3e49-44a2-a63a-5208f828d67f",
   "metadata": {},
   "source": [
    "#### A brief preview of what the call looks like\n",
    "The transcripts of these calls tend to range from 10-40 pages and 8000-30000 tokens each.\n",
    "\n",
    "For a full call see: https://abc.xyz/assets/bd/7b/d57831684953be8bcc2c5a42aee8/2024-q2-earnings-transcript.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f730386c-d2d9-44cb-a254-ab3586c65b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator: Welcome, everyone. Thank you for standing by for the Alphabet Fourth Quarter 2023 Earnings Conference Call. [Operator Instructions] \n",
      " I would now like to hand the conference over to your speaker today, Jim Friedland, Director of Investor Relations. Please go ahead. \n",
      "James Friedland: Thank you. Good afternoon, everyone, and welcome to Alphabet's Fourth Quarter 2023 Earnings Conference Call. With us today are Sundar Pichai, Philipp Schindler and Ruth Porat. \n",
      " Now I'll quickly cover the safe harbor. Some of the statements that we make today regarding our business, operations and financial performance may be considered forward-looking. Such statements are based on current expectations and assumptions that are subject to a number of risks and uncertainties. Actual results could differ materially. Please refer to our Forms 10-K and 10-Q, including the risk factors discussed in our upcoming Form 10-K filing for the year ended December 31, 2023. We undertake no obligation to update any forward-looking statement. \n",
      " During this call, we will present both GAAP and non-GAAP financial measures. A reconciliation of non-GAAP to GAAP measures is included in today's earnings press release, which is distributed and available to the public through our Investor Relations website located at abc.xyz/investor. Our comments will be on year-over-year comparisons unless we state otherwise. \n",
      " And now I'll turn the call over to Sundar. \n",
      "Sundar Pichai: Hello, everyone. Our results reflect strong momentum and product innovation continuing into 2024. Today, I'm going to talk about four main topics. One, our investments in AI, including how it's helping Search; two, subscriptions, which reached $15 billion in annual revenue, up 5x since 2019. Subscriptions is growing strongly powered by YouTube Premium and Music, YouTube TV and Google One. Three, Cloud, which crossed $9 billion in revenues this quarter and saw accelerated growth driven by our GenAI and product leadership; and four, our i\n"
     ]
    }
   ],
   "source": [
    "print(df.doc.iloc[0][:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af03247-81a3-455a-ba71-6bfca8e6e272",
   "metadata": {},
   "source": [
    "#### Create an index and upload data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "180b9ec9-846b-4901-beb1-4989e97678d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found an existing index with id=\"index_05a7cb07da764f0f81397b39ce65ab06\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'index_id': 'index_05a7cb07da764f0f81397b39ce65ab06',\n",
       " 'name': 'demo_tech_earnings_calls__v1',\n",
       " 'state': 'untrained'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = Index(API_key=API_KEY, name=\"demo_tech_earnings_calls__v1\")\n",
    "index.get_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1a02c6-876b-4e71-83c4-21fbcf10294b",
   "metadata": {},
   "source": [
    "### Upload the data\n",
    "The upload accepts a list of dictionaries (json records format). The only requirement is that the text field you wish to index is specified under the `doc` field. \n",
    "\n",
    "The upload api supports partial updates, metadata only updates. If you provide a `doc_id` field, you can update the `doc` content. If there is no `doc_id` provided, a `doc_id` is created based on the sha of the `doc` content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14427084-a203-4c7c-9aa0-f51776b31483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data to UNTRAINED index for training.\n",
      "uploading data to index...\n",
      "committing changes to index \"index_05a7cb07da764f0f81397b39ce65ab06\"..."
     ]
    }
   ],
   "source": [
    "res = index.upload(df.to_dict(\"records\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b45d4f9-a7f5-4563-a9a9-544db439f887",
   "metadata": {},
   "source": [
    "#### We can immediately query any uploaded data via standard SQL\n",
    "At the moment, this capability is not particularly interesting. However, once the unstructured data has been statistically indexed, the presently unstructured data becomes unified with the existing structured data and can be analyzed with standard quantitative analysis methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8c39e63d-fad3-4d13-b782-6cdec19266de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'quarter': '2022Q2', 'c': 1},\n",
       " {'quarter': '2022Q3', 'c': 1},\n",
       " {'quarter': '2022Q4', 'c': 2},\n",
       " {'quarter': '2023Q1', 'c': 3},\n",
       " {'quarter': '2023Q2', 'c': 5},\n",
       " {'quarter': '2023Q3', 'c': 5},\n",
       " {'quarter': '2023Q4', 'c': 5},\n",
       " {'quarter': '2024Q1', 'c': 5},\n",
       " {'quarter': '2024Q2', 'c': 4},\n",
       " {'quarter': '2024Q3', 'c': 4},\n",
       " {'quarter': '2024Q4', 'c': 2}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.queryMeta(\"SELECT quarter, count(*) as c FROM doc_meta GROUP BY quarter ORDER BY quarter\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf3938-16f1-483d-a096-8ebed121a0d0",
   "metadata": {},
   "source": [
    "### Kick of training \n",
    "No parameters are required for training. However, because our model is a hierarchical Bayesian model, we have a few levers we can pull to point our model in the right direction based on our prior knowledge about our dataset. The nature of our datset enables us to provide the model with additional information via two parameters: `doc_hierarchy` and `regex_paragraph_splitter`.\n",
    "\n",
    "`doc_hierarchy` enables the user to set the high level model structure based on existing metadata. In our case, we have earnings calls from a variety of companies and a variety of quarters. The calls likely looks vastly different across companies. Additionally, within a company calls likely look different across quarters, though I would guess not to the extent they differ across comapnies. Thus we set the model to hierarchicaly split the data first across companies, then across quarters. We are not stratifying the data. More detail, see Appendix A.\n",
    "\n",
    "`regex_paragraph_splitter` allows the user to provide regex string to split a document in a semantically meaningful way. This allows our model to index data not only across documents but also within a document. In this case, earnings call paragraphs are split by new lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f8ef6788-26ac-4fb4-9211-713336580802",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_hierarchy = [\"quarter\", \"ticker\"]\n",
    "regex_paragraph_splitter = \"\\n\"\n",
    "job = index.train(dict(doc_hierarchy=doc_hierarchy, regex_paragraph_splitter=regex_paragraph_splitter), wait=False)\n",
    "job.get_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a377e7b-aa18-492b-b0d4-26125b328702",
   "metadata": {},
   "source": [
    "## Crunching the numbers\n",
    "Because we set `wait=False`, the train functional call returned to us a job object. This job provides a mechanism to poll the job to estimate it's status. Training takes anywhere from minutes to 1-2 days, scaling linearly with the number of tokens in the dataset. \n",
    "\n",
    "We are training a custom model tuned to your data and your data alone. This is not a neural network model but a bayesian estimator that will hierarchically structure your data and orgainize into a discrete set of topics, applied to each word, sentence, paragraph, document and metadata structure you provide. This statistical structure does not hallucinate, naturally debiases language (and has configuations to debias around metadata), and opens the door to running quantitative analysis on unstructured data. \n",
    "\n",
    "Once a model is trained, all future uploads will automatically be index in the same statistical structure.\n",
    "\n",
    "While you wait, you can explore a model that has already been trained on this dataset here: https://sturdystatistics.com/analyze?folder_id=index_05a7cb07da764f0f81397b39ce65ab06&comp_fields=ticker,quarter&max_excerpts_per_doc=5&bar_plot_fields=ticker,quarter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929eba07-fe3c-4e99-aa9a-74db2d8d83bd",
   "metadata": {},
   "source": [
    "# Explore your data!\n",
    "\n",
    "Once the job is done, you can explore your data on our website's dashboard!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "135f54f6-f2ad-4048-92e7-32d3e835d9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_id: index_05a7cb07da764f0f81397b39ce65ab06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://sturdystatistics.com/analyze?folder_id=index_05a7cb07da764f0f81397b39ce65ab06&comp_fields=ticker,quarter&max_excerpts_per_doc=5&bar_plot_fields=ticker,quarter'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.wait()\n",
    "def getURL(index_id, api_key):\n",
    "    url = f\"https://sturdystatistics.com/analyze?folder_id={index_id}&comp_fields=ticker,quarter&max_excerpts_per_doc=5&bar_plot_fields=ticker,quarter\"\n",
    "    if api_key is not None:\n",
    "        url += f\"&api_key={api_key}\"\n",
    "    return url\n",
    "print(\"index_id:\", index.id)\n",
    "getURL(index.id, API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff29fe5-c4ee-40f4-9452-32b64d1c87d3",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We have a series of additional notebooks in this series\n",
    "1. Upload the data\n",
    "2. Build Visualizations\n",
    "3. Throw our dirty old RAGs: Use Topic Augmented Generation to supercharge LLMs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c8b8f-a648-476c-8bf2-c7ae3abc662b",
   "metadata": {},
   "source": [
    "### Sources\n",
    "[1] https://www.investopedia.com/terms/e/earnings-call.asp\n",
    "\n",
    "[2] http://www.stat.columbia.edu/~gelman/book/BDA3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d58557-f549-4792-9881-34cb3d874768",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd735b6-bcc2-42bf-9901-5cfe045aa7b0",
   "metadata": {},
   "source": [
    "# A.\n",
    " \n",
    "\n",
    "The `doc_hierarchy` parameter enables us to tell the model about the high level structure of the dataset. In machine learning, there is often a trade off between stratification and sample size. E.g. say you polled 50,000 individuals at random who they are voting for, as well as took their age, gender, and ethnicity. This is a massive sample size for a poll (about 50x larger than a standard poll). About as large of a polling dataset you will find. You can either lump everyone together and keep a sample size of 50,000. But that doesn't provide very much information beyond a raw number: we cannot adjust for demographic, voter turnout likelihood, state distribution etc. \n",
    "\n",
    "But let's say we want to know how women are voting. If we are following standard machine learning techniques, we need to stratify our dataset (ie throw away any data that doesn't match the sample we care about). Filtering out men reduces our sample size to ~25000. Let's say specifically we want to know how women in Pennsylvania will be voting. This reduces our dataset to 750. Lets say we want to specifically know how latina women our voting. That reduces our sample size down to ~150. Let's say we want to know how lainta women under 30 are voting. Our sample size is now down to ~30. We had a giant dataset and are trying to answer the relatively basic question \"How are latina women under 30 in Pennsylvania voting?\". \n",
    "\n",
    "You can obviously solve this problem by simply polling more. E.g. If we had a sample size of 500,000, we would now have 300 latina women in PA. Still not a great sample size and now 10x more expensive of a poll. And by the time you are finished polling, the race may have drastically changed. \n",
    "\n",
    "Another solution would be to poll to answer a specific question. If we are interested in latina women in PA under 30, let's just poll only them. But as soon as you are interested in any new questions, you have to start over from scratch.\n",
    "\n",
    "Essentially instead of working harder or spending more money, why not just use more clever statistics. Hierarchy allows one to balance aggragated knowledge with stratified breakdowns. Instead of splitting up our data across a number of different models, we can just build the structure of the data into our model\n",
    "\n",
    "In this case\n",
    "```\n",
    "                US Voters\n",
    "                /   |    \\\n",
    "State:      NY     PA    CA ...\n",
    "                 /    \\\n",
    "Gender:        Men     Women ...\n",
    "                    /    |     \\\n",
    "Ethnicity        White  Black  Latin ...\n",
    "                             /    |    \\\n",
    "Age                        18-30  30-60  60+ \n",
    "```\n",
    "\n",
    "At the lowest level of the model, you can plug in our stratified model that has computes for each class the voting preference for the most specific possible demographic. Then one level up, the model aggregates those subdivisions and computes an estimate for the demographic as a whole. E.g. get an estimate for latina women under 30 from the data, then aggregate those statistics across all age groups to estimate all latina women. \n",
    "\n",
    "However, this is where the model gets interesting. The graph is bidirectional. So information about latina women 18-30 informs the latina estimator: but the latina estimator also informs the stratified estimator about latina women under 30. As a result, our smallest estimator is able to directly benefit from data about all latina women and thus improve its confidence. If in our survey for example, we saw of the 30 latina women under 30 polled, 20 planned to vote Republican and 10 Democrat, a simple stratified model would spit out latina women are 66% likely to vote republican +-5. However, if out of 150 latina women only 60 were to vote democrate, our hierarchical model would levarage that information to temper its original prediction, though weighing each vote outside of its demographic less than it would a vote inside its demographic. Going up the chain, if we saw 450 the 750 women in PA polled, this provides additional data for our hierarchical model to leverage, though weighing it less than it would latina women specifically. This bidirectional chain of reasoning is happening in every node of this graph until the model achieves a stable equilibrium and we have successfully squeezed as much information as we possibly can out of our dataset.\n",
    "\n",
    "In our current dataset, we have earnings calls from a variety of companies and a variety of quarters. The calls likely looks vastly different across companies. Additionally, within a company calls likely look different across quarters, though I would guess not to the extent they differ across comapnies. The dataset is relatively small, 36 calls, ~400k tokens altogether. Unlike Large Language Models or Neural Networks, for our model this is plenty of data. If we wanted we could train a model from scratch on just a fraction of this dataset size. We can train on such specialized datasets because our model structures itself to squeeze as much information from language as possible. And our api provides the ability to adjust our model's default structure to leverage any knowledge you have about you dataset's metadata. \n",
    "\n",
    "```\n",
    "             All Earnings Calls\n",
    "                /   |    \\\n",
    "Company:    GOOG   AAPL  META ...\n",
    "                 /    \\\n",
    "Quarter:       24Q1   24Q2 ...\n",
    "                    /         \\\n",
    "Paragarph          P1         P2 ...\n",
    "\n",
    "Sentence ...\n",
    "\n",
    "Word ...\n",
    "```\n",
    "\n",
    "\n",
    "For more on hierarchical modelling, checkout chapter 5 in http://www.stat.columbia.edu/~gelman/book/BDA3.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc799e6-549e-4cf6-845d-3d272d6b9e81",
   "metadata": {},
   "source": [
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
